\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\renewcommand{\arraystretch}{1.5}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{Computer Architecture HW2}
\author{Fabian WÃ¼thrich}

\begin{document}

\maketitle

\section{Critical Paper Reviews [1000 points]}

see \href{https://safari.ethz.ch/review/architecture20/}{here}

\section{RowHammer [200 points]}

\subsection{RowHammer Properties}

\begin{enumerate}[label=\alph*)]
    \item True
    \item False
    \item True (i.e. protector cells)
    \item True (i.e. aggressor or protector cells)
    \item True
\end{enumerate}

\subsection{RowHammer Mitigations}

\begin{enumerate}[label=\alph*)]
    \item If the refresh interval is reduced from 64ms to 8ms, each row is refreshed 8 more times.
        Thus, bank utilization increases to $8U$ and energy consumption increases to $8E$

    \item With a doubling of the rows, the mitigation is still possible, but the bank is occupied 
        by refresh operations 80\% of the time ($U = 0.05 \times 8 \times 2 = 0.8$).

        With another doubling of the rows, the mitigation cannot be implemented because we cannot refresh every
        row in 8ms ($U = 0.05 \times 8 \times 4 = 1.6$).

    \item No, we need to know which rows are adjacent.

    \item With a 8ms refresh interval an attacker can issue a limited number of activations, constrained
        by $t_{RC}$. If $T$ is less than the maximum number of activations issued in 8ms, we
        can guarantee the same level of security. We have $t_{RC} = t_{RAS} + t_{RP} = 35ns + 13.5 ns = 48,5ns$
        and therefore
        \begin{equation*}
        T = \frac{8ms}{48.5ns} = 164948.4536 \approx 164948
        \end{equation*}

    \item A single counter requires $log_2(164948) = 17.3317 \approx 18$bits and each row needs a counter.
        Thus,
        \begin{equation*}
            18 \, bits/row \times 2^{15} \, rows/bank \times 8 \, banks \times 2 \, ranks = 9Mbit
        \end{equation*}
        When the number of rows per bank and the number of banks per chip are doubled we get
        \begin{equation*}
            18 \, bits/row \times 2^{16} \, rows/bank \times 16 \, banks \times 2 \, ranks = 36Mbit
        \end{equation*}

    \item The memory controller performs unnecessary activations which is bad for performance and
        energy consumption.

    \item Let X be a RV that describes the number of errors in a year. Each 64ms interval can be seen
        as a independent experiment of getting an error. Therefore, X has a binomial distribution with
        the parameters $p=1.9\cdot10^{-22}$ and  $n=\frac{365 \cdot 24 \cdot 3600s}{64ms} = 492'750'000$.
        Then, the probability can be calculated as follows: 
        \begin{align*}
            P(X\geq1) &= 1 - P(X=0) \\
                      &= 1 - (1 - p)^n \\
                      &= 9.3622 \cdot 10^{-14}
        \end{align*}

\end{enumerate}

\section{Processing in Memory: Ambit [200 points]}

\subsection{In-DRAM Bitmap Indices I}

\begin{enumerate}[label=\alph*)]
    \item All users occupy $\frac{u}{8}$ bytes so we need $\frac{u}{8\cdot8k}$ subarrays. Including the
        weeks, $\frac{u \cdot w}{8\cdot8k}$ rows are occupied.
    \item Using Ambit, we copy each row into the \textit{Operand} row and perform a bulk \verb|and|. This 
        requires $\frac{u \cdot w}{8\cdot8k} \times (t_{rc} + t_{and})$ seconds overall. Then, we need 
        to transfer $\frac{u}{8}$ bytes to the CPU and count the bits. This takes $\frac{u}{8X}$ seconds.
        Therefore, the throughput is
        \begin{equation*}
            \frac{u}{
                \frac{u \cdot w}{8\cdot8k} \times (t_{rc} + t_{and}) +
                \frac{u}{8X}
            } \, users/second
        \end{equation*}

    \item The CPU has to transfer all users and weeks from memory which takes $\frac{uv}{8X}$
        seconds. Therefore, the throughput is
        \begin{equation*}
            \frac{u}{\frac{uv}{8X}} = \frac{8X}{w} \, users/second
        \end{equation*}

    \item We want that the execution time on the CPU is lower than in memory. Therefore,
        \begin{equation*}
        \frac{uv}{8X} < \frac{u \cdot w}{8\cdot8k} \times (t_{rc} + t_{and}) +
                \frac{u}{8X}
        \end{equation*}
        Solving the inequality for $w$ gives
        \begin{equation*}
            w < \frac{1}{1 - \frac{X}{8k} \times (t_{rc} + t_{and})}
        \end{equation*}
\end{enumerate}

\subsection{In-DRAM Bitmap Indices II}

\begin{enumerate}[label=\alph*)]
    \item According to the cache specification we use \verb|addr[5:0]| for the offset,
        \verb|addr[22:6]| for the set index and \verb|addr[31:23]| as tag. In each loop
        iteration the addresses \verb|0x05000000+4*i|, \verb|0x06000000+4*i| and 
        \verb|0x07000000+4*i| are loaded into cache. These addresses map to the same
        set but have a different tag so they evict each other out of the cache. Thus, the
        code has only cache misses and requires $3 \times 2^{15} \times 100$ cycles.

    \item 
        \begin{enumerate}[label=\roman*)]
            \item Ambit can perform bitwise operations on the size of a row so the loop
                iterates 
                $\frac{database \, size}{row \, size} = \frac{32 \cdot 1024 \cdot 4 \, bytes}{8 \cdot 1024 \, bytes} = 16$
                times. The query can be executed as in the listing below:
\begin{lstlisting}[language=c]
for(int i = 0; i < 16; i++){
    bbop_xor 0x08000000,            0x05000000 + i*8192,    0x0A000000
    bbop_xor 0x09000000,            0x06000000 + i*8192,    0x0B000000
    bbop_or  0x07000000 + i*8192,   0x08000000 ,            0x09000000
}
\end{lstlisting}

            \item Ambit requires 25 ACTIVATE and 11 PRECHARGE commands for \verb|bbop_xor| and
                11 ACTIVATE and 5 PRECHARGE commands for \verb|bbop_or|. The query executed with
                Ambit takes $16 \times (2 \times (25 \times 50 + 11 \times 20) + (11 \times 50 + 5 \times 20))$
                cycles so the speedup is $\frac{9830400}{57440}\approx 171$x
        \end{enumerate}
\end{enumerate}

\section{In-DRAM Bit Serial Computation [200 points]}

\begin{enumerate}[label=\alph*)]
    \item The STREAM benchmark transfers two arrays with 102'400 4-bit elements and performs no computation.
        In total, $\frac{2 \times 102'400 \times 4}{8} = 102'400$ bytes are moved to the CPU. Therefore,
        \begin{equation*}
            M = \frac{num\_bytes}{t_{cpu}} = \frac{102'400}{0.00001} = 10.24 \, GB/s
        \end{equation*}
        The first experiment runs 65'536 operations (additions) and transfers 
        $\frac{3 \times 65'536 \times 4}{8}$ bytes between CPU and memory. With $M=10.24$ GB/s we get
        \begin{equation*}
            K = \frac{t_{cpu} - num\_bytes / M}{num\_operations} = \frac{0.0001 - 
            \frac{3 \times 65'536 \times 4}{8} / 10.24 \cdot 10^9}{65'536} = 1.38 \, ns/operation
        \end{equation*}
    \item The Ambit base implementation is shown in the following listing:
\begin{lstlisting}[language=C]
// S = 0x00C00000
// Cout/Cin = 0x00900000
for(int i = 0; i < num_bits_per_element; ++i) {
    bbop_xor 0x00700000,            0x00A00000+i*0x2000,    0x00B00000+i*0x2000
    bbop_xor 0x00C00000+i*0x2000,   0x00700000,             0x00900000
    boop_and 0x00700000,            0x00700000,             0x00900000
    boop_and 0x00800000,            0x00A00000+i*0x2000,    0x00B00000+i*0x2000
    boop_or  0x00900000,            0x00700000,             0x00800000
}
\end{lstlisting}

    \item We get the maximum number of operations when we use each bit of a row as an element. With a row
        size of 8KB we can perform $8 \times 1028 \times 8 = 65'536$ additions i.e. max. number of operations.
        
        Ambit requires 25 ACTIVATE and 11 PRECHARGE commands for \verb|bbop_xor| and
        11 ACTIVATE and 5 PRECHARGE commands for \verb|bbop_or| and \verb|bbop_and|. The code executed with
        Ambit takes 
        $n \times (2 \times (25 \times 20ns + 11 \times 10ns) + 3 \times (11 \times 20ns + 5 \times 10ns) = n \times 2030 \, ns$. The maximum throughput is then $\frac{65'536}{2030 \cdot n \cdot 10^{-9}}$ operations per second.

    \item As calculated in subtask (c) the Ambit-based implementation takes $2030n$ns to execute. The 
        execution time of the CPU-based implementation is 
        $1.38 \times 10^{-9} \times 65'536 + \frac{3 \times 65'536 \times n}{8 \times 10.24 \times 10^9} \approx 2.4 \times 10^{-6}$ns. 
        Thus, there is no element size that makes the CPU-based implementation faster than the Ambit 
        implementation.
\end{enumerate}

\section{Caching vs. Processing-in-Memory [200 points]}

\begin{enumerate}[label=\alph*)]
    \item The inner loop has two iterations and two memory accesses. The outer loop iterates 16 times
        and has one memory access. Therefore, the execution time is 
        $50 \times (16 + 16 \times 2 \times 2) = 4000$ cycles.
    
    \item According to the cache specification two elements fit in each cache block. The code has 
        the following access pattern per outer loop iteration:

        \textbf{1. Iteration} \newline
        Access: A[0], B[0], A[1], B[1], A[0] \newline
        H/M patter: M M M M M \newline
        Latency: $5 \times 50 = 250$ cycles \newline
        Cache: Set 0: A[0:1] \newline
        
        \textbf{2. Iteration} \newline
        Access: A[1], B[0], A[2], B[1], A[1] \newline
        H/M patter: H M M H M \newline
        Latency: $2 \times 5 + 3 \times 50 = 165$ cycles \newline
        Cache: Set 0: A[0:1] Set 1: A[2:3] \newline
\newpage
        \textbf{3. Iteration} \newline
        Access: A[2], B[0], A[3], B[1], A[2] \newline
        H/M patter: H M H H H \newline
        Latency: $4 \times 5 + 1 \times 50 = 70$ cycles \newline
        Cache: Set 0: B[0:1] Set 1: A[2:3] \newline
        
        \textbf{4. Iteration} \newline
        Access: A[3], B[0], A[4], B[1], A[3] \newline
        H/M patter: H H M H H \newline
        Latency: $4 \times 5 + 1 \times 50 = 70$ cycles \newline
        Cache: Set 0: B[0:1] Set 1: A[2:3] Set 2: A[4:5] \newline
        
        \textbf{5. Iteration} \newline
        Access: A[4], B[0], A[5], B[1], A[4] \newline
        H/M patter: H H H H H \newline
        Latency: $5 \times 5 = 25$ cycles \newline
        Cache: Set 0: B[0:1] Set 1: A[2:3] Set 2: A[4:5] \newline

        Now the single-miss and no-miss patterns are interleaved until the end.

        Latency: $250 + 165 + 70 + 7 \times 70 + 6 \times 50 = 1275$ cycles
    

    \item This is the same as the processor without a core but with a latency of 10 instead of 50 cycles.
        The new latency is 800 cycles.
    
    \item Increasing the cache size would not eliminate the conflicts between A and B in the first set,
        so she is not correct.

    \item One possibility could be to use a set-associative cache. 2-ways would eliminate the
        conflict in set 0 so only compulsory misses occur.
\end{enumerate}
\end{document}

